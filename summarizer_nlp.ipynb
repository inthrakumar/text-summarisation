{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ec2f08-a1dd-4b97-815f-b63e61551805",
      "metadata": {
        "scrolled": true,
        "id": "52ec2f08-a1dd-4b97-815f-b63e61551805"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define stop words\n",
        "stop_words = set([\"a\", \"a's\", \"able\", \"about\", \"above\", \"according\", \"accordingly\", \"across\", \"actually\", \"after\", \"afterwards\", \"again\", \"against\", \"ain't\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \"aren't\", \"around\", \"as\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\", \"away\", \"awfully\", \"b\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"both\", \"brief\", \"but\", \"by\", \"c\", \"c'mon\", \"c's\", \"came\", \"can\", \"can't\", \"cannot\", \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\", \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn't\", \"course\", \"currently\", \"d\", \"definitely\", \"described\", \"despite\", \"did\", \"didn't\", \"different\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"done\", \"down\", \"downwards\", \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \"enough\", \"entirely\", \"especially\", \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"f\", \"far\", \"few\", \"fifth\", \"first\", \"five\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\", \"from\", \"further\", \"furthermore\", \"g\", \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he's\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here's\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"ie\", \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\", \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isn't\", \"it\", \"it'd\", \"it'll\", \"it's\", \"its\", \"itself\", \"j\", \"just\", \"k\", \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"l\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"let's\", \"like\", \"liked\", \"likely\", \"little\", \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\", \"might\", \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"n\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \"often\", \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"p\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"placed\", \"please\", \"plus\", \"possible\", \"presumably\", \"probably\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\", \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\", \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\", \"sup\", \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"tell\", \"tends\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"there's\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"uucp\", \"v\", \"value\", \"various\", \"very\", \"via\", \"viz\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"welcome\", \"well\", \"went\", \"were\", \"weren't\", \"what\", \"what's\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\", \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"z\", \"zero\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc85d869-d7b1-4673-8acc-3bde53a698b8",
      "metadata": {
        "id": "bc85d869-d7b1-4673-8acc-3bde53a698b8"
      },
      "outputs": [],
      "source": [
        "# Function to clean text: remove numbers, special characters, and extra spaces\n",
        "def clean_text(text):\n",
        "    if isinstance(text, str):\n",
        "        # Remove digits\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        # Remove special characters and retain only alphabets and spaces\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        # Replace multiple spaces with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text.lower()  # Convert to lowercase for consistency\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4826bea5-29e6-4736-9cfb-043c1425f9fc",
      "metadata": {
        "id": "4826bea5-29e6-4736-9cfb-043c1425f9fc"
      },
      "outputs": [],
      "source": [
        "# Function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Function to remove stopwords\n",
        "def remove_sw(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df57a59-a5ea-479e-a7bb-d086ca4950c2",
      "metadata": {
        "id": "5df57a59-a5ea-479e-a7bb-d086ca4950c2"
      },
      "outputs": [],
      "source": [
        "# Function to calculate term frequency\n",
        "def assign_words_term_frequency(tokens_without_sw, unique_words, term_frequency_unique_words):\n",
        "    for token in tokens_without_sw:\n",
        "        if token in unique_words:\n",
        "            term_frequency_unique_words[unique_words.index(token)] += 1\n",
        "        else:\n",
        "            unique_words.append(token)\n",
        "            term_frequency_unique_words.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a699272-d43e-4538-b618-7656ac6bfb8e",
      "metadata": {
        "id": "7a699272-d43e-4538-b618-7656ac6bfb8e",
        "outputId": "0f769f0f-1297-4560-c86d-a6cb533a3cea"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Please enter the paragraph you would like to summarize:\n",
            " kjfbnrjwg 7yewgy7rt8 8wt78 ge7ft8 78wetf78tw 7y77e ewgrwg \n"
          ]
        }
      ],
      "source": [
        "# Input paragraph from user\n",
        "paragraph = input(\"Please enter the paragraph you would like to summarize:\\n\")\n",
        "\n",
        "# Preprocess the input paragraph\n",
        "cleaned_text = clean_text(paragraph)\n",
        "tokens = tokenize_text(cleaned_text)\n",
        "tokens_without_sw = remove_sw(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad7ffb3f-06dc-4de3-bdd9-6c0d6c00f52f",
      "metadata": {
        "id": "ad7ffb3f-06dc-4de3-bdd9-6c0d6c00f52f",
        "outputId": "42c40228-6d50-4e4b-9258-ffd2cb5c8894"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "Words - TF\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "kjfbnrjwg - 1\n",
            "yewgyrt - 1\n",
            "wt - 1\n",
            "geft - 1\n",
            "wetftw - 1\n",
            "ye - 1\n",
            "ewgrwg - 1\n"
          ]
        }
      ],
      "source": [
        "# Calculate term frequency\n",
        "unique_words = []\n",
        "term_frequency_unique_words = []\n",
        "assign_words_term_frequency(tokens_without_sw, unique_words, term_frequency_unique_words)\n",
        "\n",
        "# Display term frequencies\n",
        "print('\\n---------------------------------------------------------------------------\\nWords - TF\\n---------------------------------------------------------------------------\\n')\n",
        "for word, term_frequency in zip(unique_words, term_frequency_unique_words):\n",
        "    print(word, term_frequency, sep=' - ')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a1e361-1be9-4f9a-a2fd-0700db74d4d5",
      "metadata": {
        "id": "01a1e361-1be9-4f9a-a2fd-0700db74d4d5",
        "outputId": "f442da8c-8833-4a1a-9622-a552a34c6446"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "Words - Normalized Term Frequency\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "kjfbnrjwg - 1.0\n",
            "yewgyrt - 1.0\n",
            "wt - 1.0\n",
            "geft - 1.0\n",
            "wetftw - 1.0\n",
            "ye - 1.0\n",
            "ewgrwg - 1.0\n"
          ]
        }
      ],
      "source": [
        "# Normalize Term Frequency\n",
        "max_term_frequency = max(term_frequency_unique_words, default=1)  # Avoid division by zero\n",
        "normalized_term_frequency_unique_words = [tf / max_term_frequency for tf in term_frequency_unique_words]\n",
        "\n",
        "# Print the normalized term frequencies\n",
        "print('\\n---------------------------------------------------------------------------\\nWords - Normalized Term Frequency\\n---------------------------------------------------------------------------\\n')\n",
        "for word, normalized_term_frequency in zip(unique_words, normalized_term_frequency_unique_words):\n",
        "    print(f\"{word} - {normalized_term_frequency}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064fb35a-ee99-430f-a0d6-a2a0eef64b80",
      "metadata": {
        "id": "064fb35a-ee99-430f-a0d6-a2a0eef64b80"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Split paragraph into sentences for scoring\n",
        "sentences = re.split(r'(?<=[.!?])\\s+', paragraph.strip())  # Handle punctuation followed by spaces\n",
        "cleaned_sentences = [clean_text(sentence) for sentence in sentences]  # Clean each sentence\n",
        "sentence_score = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d37e0ea-c2ff-4817-ac47-6a44690bf4b1",
      "metadata": {
        "id": "1d37e0ea-c2ff-4817-ac47-6a44690bf4b1",
        "outputId": "7c9114f7-4e74-4ccb-8b1c-0936e5f2013a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Sentence - Score\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "kjfbnrjwg 7yewgy7rt8 8wt78 ge7ft8 78wetf78tw 7y77e ewgrwg ---- 7.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Score each sentence based on term frequency\n",
        "for sentence in cleaned_sentences:\n",
        "    words = tokenize_text(sentence)\n",
        "    score = sum(\n",
        "        normalized_term_frequency_unique_words[unique_words.index(word)]\n",
        "        for word in words if word in unique_words\n",
        "    )\n",
        "    sentence_score.append(score)\n",
        "\n",
        "# Print sentence scores\n",
        "print('---------------------------------------------------------------------------\\nSentence - Score\\n---------------------------------------------------------------------------\\n')\n",
        "for original_sentence, score in zip(sentences, sentence_score):\n",
        "    print(f\"{original_sentence} ---- {score}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eddd29f1-a929-4400-bd14-9f5c8178c66c",
      "metadata": {
        "id": "eddd29f1-a929-4400-bd14-9f5c8178c66c"
      },
      "outputs": [],
      "source": [
        "# Generate the summary based on top-scoring sentences\n",
        "summary = []\n",
        "count = min(3, len(sentences))  # Limit the summary to the top 3 sentences\n",
        "\n",
        "while count > 0 and max(sentence_score) > 0:\n",
        "    max_value = max(sentence_score)\n",
        "    index = sentence_score.index(max_value)\n",
        "    summary.append(cleaned_sentences[index])  # Use cleaned sentences for summary\n",
        "    sentence_score[index] = -1  # Mark the used sentence score as -1\n",
        "    count -= 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0a0915-9018-4491-be52-f6be5748a076",
      "metadata": {
        "id": "6c0a0915-9018-4491-be52-f6be5748a076",
        "outputId": "aa10f2fd-88b6-4c24-b26d-9a19b6b96ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------\n",
            "Final Summary\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "kjfbnrjwg yewgyrt wt geft wetftw ye ewgrwg\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Print the final summary\n",
        "print('---------------------------------------------------------------------------\\nFinal Summary\\n---------------------------------------------------------------------------\\n')\n",
        "print(' '.join(summary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24814854-4805-40f4-9fc7-af5a58a6ab53",
      "metadata": {
        "id": "24814854-4805-40f4-9fc7-af5a58a6ab53"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}